<p>
    As the world rushes to embrace new technologies such as deep learning 
    and data-based decision making to automate and streamline processes,
    the next great challenge is keeping people in the loop.  In many
    application domains involving any risk (financial, health, welfare
    or other), humans serve as the ultimate arbiter of whether a decision
    made or recommended by a machine is based on sound reasoning-is well
    justified, correct, and fair. 
</p>
<p>
    The problem is, data-based decision-making often relies on fusing
    and processing data-based evidence in new kinds of complex ways, 
    substituting data-based evidence for traditionally human-led 
    interpretation and observation.  Thus ascertaining the aforementioned
    objectives can be extremely challenging, and requires thinking of 
    new kinds of tools and techniques to "unveil" the essence of how
    machine-based decisions are made.  In some cases, this also requires
    us to re-think the machine learning classifiers and analytical methods
    used altogether.
</p>
<p>
    In this set of projects we investigate the range of kinds of 
    techniques we can apply to empower people responsible for making
    data-supported decisions empowered to understand the problems
    and challengest that arise from complex data-based processes.
    We look at various approaches: interpretable ML, approximate 
    explanations, among others.
</p>
