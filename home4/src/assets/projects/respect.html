<p>
    Our world is now full of algorithms; these include algorithms that do things
    <i>to us</i> (such as news feed recommenders and targeted advertising engines), 
    and those that do things <i>for us</i> (such as virtual assistants).  
    Some of these are designed to serve our needs exclusively, while others are designed
    to serve others' needs at our expense.  
</p>
<p>
    As we start delegating imore and more tasks to algorithms, there are circumstances in 
    which it is important to know <i>whom the algorithm is serving</i> to be able to 
    anticipate how it will behave.  Such is an important element of trust: if we are not
    able to anticipate what something will do or why, we will nautrally be distinclined to 
    trust that it will do the right thing.
</p>
<p>
    In this philosophical work, we propose a new criterion to differentiate algorithms
    that serve their users from those that are designed to serve others' needs. We
    call this <i>respect</i>-an algorithm respects a user if it genuinely serves their
    needs.  Following the argument by Immanuel Kant, respect for persons is the basis of murality; 
    a person is moral if they treat others with respect.
</p>
<p> 
    We propose that Respect as a notion can be compared to Fairness, Transparency, Accountability
    which are properties common to algorithms investigated by the FAT*/FAT-ML research
    community. 
</p>

<!-- <p>
    Such algorithms pre-date the Internet; for instance, algorithms by HMRC/the IRS 
    run to ascertain whether a citizen has committed tax fraud on behalf ot the 
    government, can dramatically  -->